{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries and dependencies needs to be installed via conda or pip\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from time import perf_counter \n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import osmnx as ox \n",
    "from collections import Counter\n",
    "import pygal\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, time\n",
    "import matplotlib\n",
    "\n",
    "#Interactive visualization\n",
    "import folium\n",
    "from folium import plugins\n",
    "# Adds tool to the top right\n",
    "from folium.plugins import MeasureControl\n",
    "from folium.plugins import FastMarkerCluster\n",
    "import branca.colormap as cm\n",
    "from IPython.display import HTML\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import MultiPoint\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn import metrics\n",
    "\n",
    "#from pygeocoder import Geocoder \n",
    "\n",
    "import reverse_geocoder as rg #info. on output:(name: cityname, admin1:state, admin2:county, cc:country code): https://github.com/thampiman/reverse-geocoder\n",
    "import pprint \n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from plotly import tools\n",
    "import matplotlib.cm as cm\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import random\n",
    "\n",
    "from kneed.data_generator import DataGenerator\n",
    "from kneed.knee_locator import KneeLocator\n",
    "\n",
    "import hdbscan\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  # To Compute Haversine distance\n",
    "def sphere_dist(o_lat, o_lon, d_lat, d_lon):\n",
    "    \"\"\"\n",
    "    Return distance along great radius between origin and destination coordinates.\n",
    "    \"\"\"\n",
    "    #Define earth radius (km)\n",
    "    R_earth = 6371\n",
    "    #Convert degrees to radians\n",
    "    o_lat, o_lon, d_lat, d_lon = map(np.radians,\n",
    "                                                             [o_lat, o_lon, \n",
    "                                                              d_lat, d_lon])\n",
    "    #Compute distances along lat, lon dimensions\n",
    "    dlat = d_lat - o_lat\n",
    "    dlon = d_lon - o_lon\n",
    "    \n",
    "    #Compute haversine distance\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(o_lat) * np.cos(d_lat) * np.sin(dlon/2.0)**2\n",
    "    return 2 * R_earth * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def radian_conv(degree):\n",
    "    \"\"\"\n",
    "    Return radian.\n",
    "    \"\"\"\n",
    "    return  np.radians(degree) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Courtesy of https://stackoverflow.com/questions/26079881/kl-divergence-of-two-gmms. Here the difference is that we take the squared root, so it's a proper metric\n",
    "\n",
    "def gmm_js(gmm_p, gmm_q, n_samples=10**5):\n",
    "    X = gmm_p.sample(n_samples)[0]\n",
    "    log_p_X = gmm_p.score_samples(X)\n",
    "    log_q_X = gmm_q.score_samples(X)\n",
    "    log_mix_X = np.logaddexp(log_p_X, log_q_X)\n",
    "\n",
    "    Y = gmm_q.sample(n_samples)[0]\n",
    "    log_p_Y = gmm_p.score_samples(Y)\n",
    "    log_q_Y = gmm_q.score_samples(Y)\n",
    "    log_mix_Y = np.logaddexp(log_p_Y, log_q_Y)\n",
    "    \n",
    "    return np.sqrt((log_p_X.mean() - (log_mix_X.mean() - np.log(2))\n",
    "            + log_q_Y.mean() - (log_mix_Y.mean() - np.log(2))) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Training Dataset for the Model\n",
    "\n",
    "def ReadFromCSV(FilePath, sckipLines = 0, sep = ','):  \n",
    "\n",
    "    #Read and clean the csv file in FilePath ignoring the first sckipLines lines.\n",
    "\n",
    "    #(omits non-ascii characters from columns' name)\n",
    "\n",
    "    xa = pd.read_csv(FilePath, header=sckipLines, sep = sep, low_memory=False)\n",
    "\n",
    "    return xa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_manipulation(J):\n",
    "    \n",
    "    ## clean the data from non meaningful lat. and long.\n",
    "    # ##### Valid range of latitude and longitude for India: lat range( 8°4′ N to 37°6′ N) & long. range( 68°7′ E to 97°25′ E)\n",
    "    \n",
    "    J=J[(J['origin_lat'] >= 8) & (J['origin_lat'] <= 38)  & (J['destination_lat'] >= 8) & (J['destination_lat']<=38)]\n",
    "    J=J[(J['origin_lng'] >= 67) & (J['origin_lng'] <= 98)  & (J['destination_lng'] >= 67) & (J['destination_lng']<=98)]                            \n",
    "               \n",
    "    J_clean=J.loc[(J['origin_lng'] >= 67) & (J['destination_lng'] <= 98)]\n",
    "    \n",
    "    \n",
    "    # ### Jitter in lat/long of cols ==> The second decimal place is worth up to 1.1 km\n",
    "\n",
    "    J_clean['origin_lat']=J_clean['origin_lat'].map(lambda x: x +np.random.rand()/5.0)\n",
    "    J_clean['origin_lng']=J_clean['origin_lng'].map(lambda x: x +np.random.rand()/5.0)\n",
    "\n",
    "    J_clean['destination_lat']=J_clean['destination_lat'].map(lambda x: x +np.random.rand()/5.0)\n",
    "    J_clean['destination_lng']=J_clean['destination_lng'].map(lambda x: x +np.random.rand()/5.0)\n",
    "    \n",
    "    J_clean=J_clean.dropna() #drop rows with missing values\n",
    "    \n",
    "    J_clean['distance'] = sphere_dist(J_clean['origin_lat'], J_clean['origin_lng'], \n",
    "                                   J_clean['destination_lat'] , J_clean['destination_lng'])\n",
    "    J_clean[\"datetime\"] = pd.to_datetime(J_clean[\"datetime\"]) #for time series needed to be dtype: datetime64[ns, UTC]\n",
    "    \n",
    "    J_clean['month']=J_clean['datetime'].dt.month\n",
    "    \n",
    "    return J_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Long trip shipment or short trip shipment distances\n",
    "\n",
    "def vis_route_length_dist(d):\n",
    "\n",
    "    route_lengths=d['distance']\n",
    "\n",
    "    long_routes = len([k for k in route_lengths if k > 400]) / len(route_lengths)\n",
    "    medium_routes = len([k for k in route_lengths if k < 400 and k > 300]) / len(route_lengths)\n",
    "    short_routes = len([k for k in route_lengths if k < 300]) / len(route_lengths)\n",
    "\n",
    "\n",
    "    chart = pygal.HorizontalBar()\n",
    "    chart.title = 'Long, medium, and short routes'\n",
    "    chart.add('Long', long_routes * 100)\n",
    "    chart.add('Medium', medium_routes * 100)\n",
    "    chart.add('Short', short_routes * 100)\n",
    "    \n",
    "    return chart\n",
    "#vis_route_length_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #####. Function of find optimal number of clusters (GMM components)? Based on BIC & Silhouetee score\n",
    "\n",
    "\n",
    "def optimal_cluster_k (X):\n",
    "    \n",
    "    if X.shape[0]==1 or X.shape[0]==2 : #125 instances in this category\n",
    "        \n",
    "        n_clusters=1\n",
    "    \n",
    "    elif 2< X.shape[0] <= 10:  #89 instances in this category\n",
    "        \n",
    "        n_clusters=2\n",
    "        \n",
    "        gmm = GaussianMixture(n_components=n_clusters, covariance_type='full',random_state=42).fit(X)\n",
    "        cluster_labels = gmm.predict(X)\n",
    "        \n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "\n",
    "        print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", silhouette_avg)    \n",
    "        \n",
    "    elif  11<= X.shape[0] <= 150:  #172 instances in this category\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            k_clusters= np.arange(2, X.shape[0]-1,1)\n",
    "            models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(X) for n in k_clusters]\n",
    "            kn = KneeLocator(k_clusters, [m.bic(X) for m in models], curve='convex', direction='decreasing')\n",
    "            knee_k = kn.knee\n",
    "\n",
    "            #print (knee_k)\n",
    "            bic_min = min([m.bic(X) for m in models])\n",
    "            k_bic=k_clusters[[m.bic(X) for m in models].index(bic_min)]\n",
    "            #print (k_bic)\n",
    "\n",
    "            plt.plot(k_clusters, [m.bic(X) for m in models], label='BIC') \n",
    "            plt.plot(k_clusters, [m.aic(X) for m in models], label='AIC') \n",
    "            plt.vlines(x=knee_k, ymin=min([m.bic(X)-500 for m in models]), ymax=max([m.bic(X) for m in models]), linestyle='--')\n",
    "            plt.legend(loc='best')\n",
    "            plt.xlabel('n_components', fontsize=15);\n",
    "\n",
    "            range_n_clusters = np.arange(knee_k-2, X.shape[0]-1,1)\n",
    "\n",
    "            for n_clusters in range_n_clusters:\n",
    "\n",
    "                # Initialize the clusterer with n_clusters value and a random generator\n",
    "                # seed of 10 for reproducibility.\n",
    "                #clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "                #cluster_labels = clusterer.fit_predict(X)\n",
    "                gmm = GaussianMixture(n_components=n_clusters, covariance_type='full',random_state=42).fit(X)\n",
    "                cluster_labels = gmm.predict(X)\n",
    "                #print (cluster_labels)\n",
    "                # The silhouette_score gives the average value for all the samples.\n",
    "                # This gives a perspective into the density and separation of the formed\n",
    "                # clusters\n",
    "\n",
    "                silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        \n",
    "                if silhouette_avg > 0.45: #last changed from 0.55 to 0.45 to relax it\n",
    "                    print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", silhouette_avg)\n",
    "                    \n",
    "                    break\n",
    "        except:\n",
    "            print (\"Failed to find optimal number of clusters\")\n",
    "\n",
    "    elif X.shape[0] > 150: #72 instances in this category\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            k_clusters= np.arange(2, 120,10)\n",
    "            models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(X) for n in k_clusters]\n",
    "            kn = KneeLocator(k_clusters, [m.aic(X) for m in models], curve='convex', direction='decreasing')\n",
    "            knee_k = kn.knee\n",
    "            #print (knee_k)\n",
    "\n",
    "            bic_min = min([m.bic(X) for m in models])\n",
    "            k_bic=k_clusters[[m.bic(X) for m in models].index(bic_min)]\n",
    "\n",
    "            #print (k_bic)\n",
    "\n",
    "    #         plt.plot(k_clusters, [m.bic(X) for m in models], label='BIC') \n",
    "    #         plt.plot(k_clusters, [m.aic(X) for m in models], label='AIC') \n",
    "    #         plt.vlines(x=knee_k, ymin=min([m.bic(X)-500 for m in models]), ymax=max([m.bic(X) for m in models]), linestyle='--')\n",
    "    #         plt.legend(loc='best')\n",
    "    #         plt.xlabel('n_components', fontsize=15);\n",
    "\n",
    "            range_n_clusters = np.arange(knee_k-2, 120,2) \n",
    "\n",
    "            for n_clusters in range_n_clusters:\n",
    "\n",
    "                # Initialize the clusterer with n_clusters value and a random generator\n",
    "                # seed of 10 for reproducibility.\n",
    "                #clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "                #cluster_labels = clusterer.fit_predict(X)\n",
    "                gmm = GaussianMixture(n_components=n_clusters, covariance_type='full',random_state=42).fit(X)\n",
    "                cluster_labels = gmm.predict(X)\n",
    "\n",
    "                # The silhouette_score gives the average value for all the samples.\n",
    "                # This gives a perspective into the density and separation of the formed\n",
    "                # clusters\n",
    "                silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "\n",
    "\n",
    "                if silhouette_avg > 0.45:\n",
    "                    print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", silhouette_avg)    \n",
    "                    break\n",
    "        except:\n",
    "            print (\"Failed to find optimal number of clusters\")\n",
    "    \n",
    "    return n_clusters \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_Model(df_train,outputpath):\n",
    "\n",
    "    #create new column to store origin state_destination state combination\n",
    "    df_train['shipments_states']=df_train.apply(lambda x:'%s_%s' % (x['origin_state'],x['destination_state']),axis=1) \n",
    "\n",
    "    #make a list from all the route in each state_state shipments\n",
    "    routes_states = df_train['shipments_states'].unique().tolist()\n",
    "\n",
    "    #create a dictionary to store all dataframes that each stores the single origin state_destination state\n",
    "    df_train_dict = {state_state: df_train.loc[df_train['shipments_states'] == state_state] for state_state in routes_states}\n",
    "\n",
    "\n",
    "    k_list=[]\n",
    "    gmm_tr={}\n",
    "\n",
    "    for k, v in df_train_dict.items():\n",
    "        try:\n",
    "\n",
    "            if df_train_dict[k].shape[0]==1:\n",
    "                print (k)\n",
    "                print ('This route is single route and one cluster')\n",
    "            else:\n",
    "                print (k)\n",
    "                coords_tr = df_train_dict[k].as_matrix(columns=['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng'])\n",
    "\n",
    "                optim_tr= optimal_cluster_k(coords_tr)\n",
    "\n",
    "\n",
    "                gmm_tr[k]= GaussianMixture(n_components=optim_tr, covariance_type='full',random_state=42).fit(coords_tr)\n",
    "\n",
    "                cluster_labels_tr = gmm_tr[k].predict(coords_tr)      \n",
    "                probs_tr = gmm_tr[k].predict_proba(coords_tr)                  \n",
    "\n",
    "                df_train_dict[k]['route_zones']= pd.Series(cluster_labels_tr, index=df_train_dict[k].index)\n",
    "                df_train_dict[k]['probs_route_zone'] = pd.Series(probs_tr.max(1), index=df_train_dict[k].index)\n",
    "                df_train_dict[k].to_csv(outputpath + k + '.csv')\n",
    "\n",
    "        except:\n",
    "            print ('Unable to find optimal number of clusters and fail to cluster')\n",
    "            \n",
    "    print ('k_list:',k_list)\n",
    "    \n",
    "    return df_train_dict\n",
    "\n",
    "    \n",
    "t1_start = perf_counter()\n",
    "t1_stop = perf_counter() \n",
    "   \n",
    "print(\"Elapsed time during the whole program in minutes:\",(t1_stop-t1_start)/60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For visualization \n",
    "\n",
    "### get subset of state to state route which you want to visualize after building the model\n",
    "\n",
    "def display(ma, height=300):\n",
    "    \"\"\"Takes a folium instance and embed HTML.\"\"\"\n",
    "    ma._build_map()\n",
    "    srcdoc = ma.HTML.replace('\"', '&quot;')\n",
    "    embed = HTML('<iframe srcdoc=\"{0}\" '\n",
    "                 'style=\"width: 100%; height: {1}px; '\n",
    "                 'border: none\"></iframe>'.format(srcdoc, height))\n",
    "    return embed\n",
    "\n",
    "def state_state_clusters(ss,df_dict, output_html):\n",
    "\n",
    "    #ss='West Bengal_Telangana' ## input the origin state to destination state with this example format 'Telangana_West Bengal'\n",
    "                                                              \n",
    "    # # mark each origin as a point\n",
    "    df_plot=df_dict[ss] ## \n",
    "\n",
    "    all_routes = df_plot['route_zones'].unique().tolist()\n",
    "\n",
    "    df_dict_zones = {route_z: df_plot.loc[df_plot['route_zones'] == route_z] for route_z in all_routes}\n",
    "\n",
    "    map_m = folium.Map([16.7, 81.095], zoom_start=11,control_scale = True)\n",
    "\n",
    "\n",
    "    #create a color map\n",
    "    #color_var = 'route_zones' #what variable will determine the color\n",
    "    cmap = cm.LinearColormap(['blue', 'red','green', 'orange','pink','black', 'beige', 'white','lightblue','purple','gray','cadetblue'],\n",
    "                         vmin=df_plot['route_zones'].quantile(0.05), vmax=df_plot['route_zones'].quantile(0.95),\n",
    "                         caption = 'Cluster Labels')\n",
    "\n",
    "\n",
    "\n",
    "    FastMarkerCluster(list(zip(df_plot['origin_lat'], df_plot['origin_lng']))).add_to(map_m)\n",
    "    FastMarkerCluster(list(zip(df_plot['destination_lat'], df_plot['destination_lng']))).add_to(map_m)\n",
    "    folium.LayerControl().add_to(map_m)\n",
    "    \n",
    "    #Add the color map legend to your map\n",
    "    map_m.add_child(cmap)\n",
    "    folium.LayerControl().add_to(map_m)\n",
    "    #Add the measurement control bar\n",
    "    map_m.add_child(MeasureControl())\n",
    "\n",
    "    \n",
    "        ####### Source info. on Folium https://www.kaggle.com/rachan/how-to-folium-for-maps-heatmaps-time-analysis\n",
    "    for k, v in df_dict_zones.items():\n",
    "\n",
    "            for index, row in df_dict_zones[k].iterrows():#folium.Marker([lat, lon], popup=str(name)+': '+color+'-'+str(clname), icon=folium.Icon(color=color)).add_to(feature_group)\n",
    "\n",
    "                folium.Marker([row['origin_lat'], row['origin_lng']],\n",
    "                                    radius=10,fill=True,popup='Origin', # Set fill to True\n",
    "                                    fill_color=\"green\",fill_opacity=0.7,icon = folium.Icon(color='green'),icon_size=(1, 1)).add_to(map_m)\n",
    "\n",
    "            for index, row in df_dict_zones[k].iterrows():\n",
    "\n",
    "                folium.Marker([row['destination_lat'], row['destination_lng']],\n",
    "                                    radius=7,\n",
    "                                    fill=True, # Set fill to True\n",
    "                                    color = 'red',\n",
    "                                    fill_opacity=0.7).add_to(map_m)\n",
    "\n",
    "                folium.PolyLine([[row['origin_lat'], row['origin_lng']], [row['destination_lat'], row['destination_lng']]],color=cmap(row['route_zones']),weight=1.5, opacity=0.5).add_to(map_m)\n",
    "\n",
    "    legend_html =   '''\n",
    "                    <div style=\"position: fixed; \n",
    "                                bottom: 50px; right: 50px; width: 150px; height: 90px; \n",
    "                                border:2px solid grey; z-index:9999; font-size:14px;\n",
    "                                \">&nbsp; Legend <br>\n",
    "                                  &nbsp; Origin &nbsp; <i class=\"fa fa-map-marker fa-2x\" style=\"color:green\"></i><br>\n",
    "                                  &nbsp; Destination &nbsp; <i class=\"fa fa-map-marker fa-2x\" style=\"color:blue\"></i>\n",
    "                    </div>\n",
    "                    ''' \n",
    "    map_m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "    map_m.save(output_html+'.html')\n",
    "    \n",
    "    return map_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Check the robustness of a model with shuffle data for specific state_state route\n",
    "\n",
    "def Seasonality_check(ss, df, outputpath): #ss='West Bengal_Telangana'\n",
    "\n",
    "    df_tr=df\n",
    "\n",
    "    df_p_h1 = df_tr.loc[(df_tr['month'])<=6]\n",
    "    df_p_h2 = df_tr.loc[(df_tr['month'])>=7]\n",
    "\n",
    "    df_check_p=df_p_h1\n",
    "    df_check_q=df_p_h2\n",
    "     \n",
    "    ## input the origin state to destination state with this example format 'Telangana_West Bengal'\n",
    "    \n",
    "    df_check_p['shipments_states']=df_check_p.apply(lambda x:'%s_%s' % (x['origin_state'],x['destination_state']),axis=1)\n",
    "    #routes_states = df_check_p['shipments_states'].unique().tolist()\n",
    "    routes_states = [ss]\n",
    "\n",
    "    df_dict_p = {state_state: df_check_p.loc[df_check_p['shipments_states'] == state_state] for state_state in routes_states}\n",
    "\n",
    "\n",
    "    df_check_q['shipments_states']=df_check_q.apply(lambda x:'%s_%s' % (x['origin_state'],x['destination_state']),axis=1)\n",
    "    df_dict_q = {state_state: df_check_q.loc[df_check_q['shipments_states'] == state_state] for state_state in routes_states}\n",
    "\n",
    "\n",
    "    p_list=[]\n",
    "    q_list=[]\n",
    "    k_list=[]\n",
    "    JSD=[]\n",
    "    gmm_p={}\n",
    "    gmm_q={}\n",
    "    js_dist={}\n",
    "\n",
    "    for (k,v), (k2,v2) in zip(df_dict_p.items(), df_dict_q.items()):\n",
    "\n",
    "        if df_dict_p[k].shape[0]==1 & df_dict_q[k2].shape[0]==1:\n",
    "\n",
    "            print ('This route is single route and one cluster')\n",
    "            \n",
    "        else:\n",
    "\n",
    "            coords_p = df_dict_p[k].as_matrix(columns=['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng'])\n",
    "            coords_q = df_dict_q[k2].as_matrix(columns=['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng'])\n",
    "\n",
    "            optim_p= optimal_cluster_k(coords_p)\n",
    "            optim_q= optimal_cluster_k(coords_q)\n",
    "\n",
    "            gmm_p[k]= GaussianMixture(n_components=optim_p, covariance_type='full',random_state=42).fit(coords_p)\n",
    "            gmm_q[k2]= GaussianMixture(n_components=optim_q, covariance_type='full',random_state=42).fit(coords_q)\n",
    "\n",
    "            cluster_labels_p = gmm_p[k].predict(coords_p)\n",
    "            cluster_labels_q = gmm_p[k2].predict(coords_q)\n",
    "\n",
    "            probs_p = gmm_p[k].predict_proba(coords_p)\n",
    "            probs_q = gmm_q[k2].predict_proba(coords_q) \n",
    "\n",
    "            js_dist[k]=gmm_js(gmm_p[k], gmm_q[k2], n_samples=100)\n",
    "\n",
    "            if optim_p!= optim_q:\n",
    "\n",
    "                p_list.append(optim_p)\n",
    "                q_list.append(optim_q)\n",
    "                k_list.append(k)\n",
    "                JSD.append(js_dist[k])            \n",
    "\n",
    "            #print ('js',js_dist)\n",
    "            df_dict_p[k]['route_zones']= pd.Series(cluster_labels_p, index=df_dict_p[k].index)\n",
    "            df_dict_p[k]['probs_route_zone'] = pd.Series(probs_p.max(1), index=df_dict_p[k].index)\n",
    "            \n",
    "            df_dict_q[k]['route_zones']= pd.Series(cluster_labels_q, index=df_dict_q[k].index)\n",
    "            df_dict_q[k]['probs_route_zone'] = pd.Series(probs_q.max(1), index=df_dict_q[k].index)\n",
    "            \n",
    "            df_dict_p[k].to_csv(outputpath+k +'h1'+ '.csv')\n",
    "            df_dict_q[k].to_csv(outputpath+k +'h2'+'.csv')\n",
    "            \n",
    "    return\n",
    "\n",
    "print ('p_list:', p_list)\n",
    "print ('q_list:', q_list)\n",
    "print ('k_list:',k_list)\n",
    "print ('JS distance:', js_dist[k])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HDBSCAN_Model(df_train,outputpath):\n",
    "\n",
    "    #create new column to store origin state_destination state combination\n",
    "    df_train['shipments_states']=df_train.apply(lambda x:'%s_%s' % (x['origin_state'],x['destination_state']),axis=1) \n",
    "\n",
    "    #make a list from all the route in each state_state shipments\n",
    "    routes_states = df_train['shipments_states'].unique().tolist()\n",
    "\n",
    "    #create a dictionary to store all dataframes that each stores the single origin state_destination state\n",
    "    df_train_dict = {state_state: df_train.loc[df_train['shipments_states'] == state_state] for state_state in routes_states}\n",
    "\n",
    "\n",
    "    k_list=[]\n",
    "    hdbscan_tr={}\n",
    "    n_clusters=[]\n",
    "    \n",
    "    for k, v in df_train_dict.items():\n",
    "        try:\n",
    "\n",
    "            if df_train_dict[k].shape[0]==1:\n",
    "                print (k)\n",
    "                print ('This route is single route and one cluster')\n",
    "            else:\n",
    "                print (k)\n",
    "                coords_tr = df_train_dict[k].as_matrix(columns=['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng'])\n",
    "\n",
    "                clusterer = hdbscan.HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True,\n",
    "                leaf_size=40,min_cluster_size=10, gen_min_span_tree=True).fit(coords_tr)\n",
    "\n",
    "                cluster_labels_tr = clusterer.labels_\n",
    "                probs_tr=clusterer.probabilities_\n",
    "                outlier_score=clusterer.outlier_scores_\n",
    "                #hierarchy = clusterer.cluster_hierarchy_\n",
    "                #alt_labels = hierarchy.get_clusters(0.100, 5)\n",
    "                #hierarchy.plot()  \n",
    "                df_train_dict[k]['route_zones']= pd.Series(cluster_labels_tr, index=df_train_dict[k].index)\n",
    "\n",
    "                n_cl=df_train_dict[k]['route_zones'].max()\n",
    "                n_clusters.append(n_cl)\n",
    "                df_train_dict[k]['probs_route_zone'] = pd.Series(probs_tr.max(), index=df_train_dict[k].index)\n",
    "                df_train_dict[k]['outlier_score'] = pd.Series(outlier_score, index=df_train_dict[k].index)\n",
    "                df_train_dict[k].to_csv('/Results2_hdbscan/'+k + '.csv')\n",
    "\n",
    "        except:\n",
    "            print ('Unable to find optimal number of clusters and fail to cluster')\n",
    "            \n",
    "    print ('k_list:',k_list)\n",
    "    \n",
    "    return df_train_dict[k]\n",
    "\n",
    "#print ('k_list:',k_list)\n",
    "#print ('n_clusters:',n_clusters)\n",
    "\n",
    "   \n",
    "t1_start = perf_counter()\n",
    "t1_stop = perf_counter() \n",
    "   \n",
    "print(\"Elapsed time during the whole program in minutes:\",(t1_stop-t1_start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Check the robustness of a model with shuffle data for specific state_state route\n",
    "\n",
    "def robustness_check(ss, df, outputpath): #ss='West Bengal_Telangana'\n",
    "\n",
    "    df_tr=df\n",
    "    \n",
    "    ## input the origin state to destination state with this example format 'Telangana_West Bengal'\n",
    "    \n",
    "    df_tr['shipments_states']=df_tr.apply(lambda x:'%s_%s' % (x['origin_state'],x['destination_state']),axis=1)\n",
    "    \n",
    "    #routes_states = df_check_p['shipments_states'].unique().tolist()\n",
    "    routes_states = [ss]\n",
    "\n",
    "    df_dict_tr = {state_state: df_tr.loc[df_tr['shipments_states'] == state_state] for state_state in routes_states}\n",
    "    \n",
    "    ### test robustness with 90% of data \n",
    "    df_dict_test = {state_state: df_tr.sample(n= int(0.9*df_tr.shape[0])).loc[df_tr['shipments_states'] == state_state] for state_state in routes_states}\n",
    "\n",
    "\n",
    "    tr_list=[]\n",
    "    test_list=[]\n",
    "    \n",
    "    k_list=[]\n",
    "    silhoutee=[]\n",
    "    gmm_tr={}\n",
    "    gmm_test={}\n",
    "\n",
    "    for (k,v), (k2,v2) in zip(df_dict_tr.items(), df_dict_test.items()):\n",
    "\n",
    "        if df_dict_tr[k].shape[0]==1 & df_dict_test[k2].shape[0]==1:\n",
    "\n",
    "            print ('This route is single route and one cluster')\n",
    "            \n",
    "        else:\n",
    "\n",
    "            coords_tr = df_dict_tr[k].as_matrix(columns=['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng'])\n",
    "            coords_test = df_dict_test[k2].as_matrix(columns=['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng'])\n",
    "\n",
    "            optim_tr= optimal_cluster_k(coords_tr)\n",
    "            optim_test= optimal_cluster_k(coords_test)\n",
    "\n",
    "            gmm_tr[k]= GaussianMixture(n_components=optim_tr, covariance_type='full',random_state=42).fit(coords_tr)\n",
    "            gmm_test[k2]= GaussianMixture(n_components=optim_test, covariance_type='full',random_state=42).fit(coords_test)\n",
    "\n",
    "            cluster_labels_tr = gmm_tr[k].predict(coords_tr)\n",
    "            cluster_labels_test = gmm_test[k2].predict(coords_test)\n",
    "\n",
    "            probs_tr = gmm_tr[k].predict_proba(coords_tr)\n",
    "            probs_test = gmm_test[k2].predict_proba(coords_test) \n",
    "            \n",
    "            silhouette_avg_tr = silhouette_score(coords_tr, cluster_labels_tr)\n",
    "            silhouette_avg_test = silhouette_score(coords_test, cluster_labels_test)\n",
    "            #js_dist[k]=gmm_js(gmm_p[k], gmm_q[k2], n_samples=100) #Jenson-shanon distance\n",
    "            \n",
    "            \n",
    "            print (\"with n_clusters:\", optim_tr, 'silhouette_avg_tr is:', silhouette_avg_tr )\n",
    "            print (\"with n_clusters:\", optim_test,'silhouette_avg_test is:', silhouette_avg_test )\n",
    "            \n",
    "            if silhouette_avg_test<0.4: #silhouette_avg_tr set to be higher than 0.5, with 10% data reduction how much it will drop return the ones\n",
    "\n",
    "                tr_list.append(optim_tr)\n",
    "                test_list.append(optim_test)\n",
    "                k_list.append(k)\n",
    "                silhoutee.append(silhouette_avg_test)            \n",
    "                \n",
    "                print (\"Model needs to be retrain to have reasonable partitioning (silhouette>=0.4)\")\n",
    "                \n",
    "            df_dict_tr[k]['route_zones']= pd.Series(cluster_labels_tr, index=df_dict_tr[k].index)\n",
    "            df_dict_tr[k]['probs_route_zone'] = pd.Series(probs_tr.max(1), index=df_dict_tr[k].index)\n",
    "            \n",
    "            df_dict_test[k]['route_zones']= pd.Series(cluster_labels_test, index=df_dict_test[k].index)\n",
    "            df_dict_test[k]['probs_route_zone'] = pd.Series(probs_test.max(1), index=df_dict_test[k].index)\n",
    "            \n",
    "            df_dict_tr[k].to_csv(outputpath+k +'all_data'+ '.csv')\n",
    "            df_dict_test[k].to_csv(outputpath+k +'90perc_data'+'.csv')\n",
    "            \n",
    "    return\n",
    "\n",
    "# print ('tr_list:', tr_list)\n",
    "# print ('test_list:', test_list)\n",
    "# print ('k_list:',k_list)\n",
    "# print ('silhoutee score:', sil_dist[k])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    df_all=ReadFromCSV('/Users/mojganmazouchi/Desktop/route/all_data.csv') \n",
    "    df=data_manipulation(df_all) #(107498, 11)\n",
    "    df_train_dict=GMM_Model(df,'/Final/')\n",
    "    #robustness_check('West Bengal_Tripura', df, '/robustness/') #West Bengal_Tripura\n",
    "    map_m=state_state_clusters(ss='West Bengal_Tripura',df_dict=df_train_dict, output_html='/Visual/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_dict['West Bengal_Tripura']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Check if BIC is fixed or can change due to local opitmas for random sample of state_state pairs #####\n",
    "\n",
    "def SelBest(arr:list, X:int)->list:\n",
    "    '''\n",
    "    returns the set of X configurations with shorter distance\n",
    "    '''\n",
    "    dx=np.argsort(arr)[:X]\n",
    "    return arr[dx]\n",
    "\n",
    "coords_tr = df_train_dict['Karnataka_Uttar Pradesh'].as_matrix(columns=['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng'])\n",
    "\n",
    "X=coords_tr\n",
    "\n",
    "k_clusters= np.arange(2, int((X.shape[0]-1)/60),1)\n",
    "\n",
    "bics=[]\n",
    "bics_err=[]\n",
    "iterations=20\n",
    "\n",
    "for n in k_clusters:\n",
    "    tmp_bic=[]\n",
    "    for _ in range(iterations):\n",
    "        tmp_bic.append(GaussianMixture(n, random_state=42).fit(X).bic(X))\n",
    "    val=np.mean(SelBest(np.array(tmp_bic), int(iterations/5)))\n",
    "    err=np.std(tmp_bic)\n",
    "    bics.append(val)\n",
    "    bics_err.append(err)\n",
    "print (tmp_bic)\n",
    "\n",
    "#Check if Silhouetee score is fixed or can change due to local opitmas for random sample of state_state pairs #####\n",
    "\n",
    "n_clusters=np.arange(2, 20)\n",
    "sils=[]\n",
    "sils_err=[]\n",
    "iterations=20\n",
    "for n in n_clusters:\n",
    "    tmp_sil=[]\n",
    "    for _ in range(iterations):\n",
    "        gmm=GaussianMixture(n, random_state=42).fit(X)\n",
    "        labels=gmm.predict(X)\n",
    "        sil=metrics.silhouette_score(X, labels, metric='euclidean')\n",
    "        tmp_sil.append(sil)\n",
    "    val=np.mean(SelBest(np.array(tmp_sil), int(iterations/5)))\n",
    "    err=np.std(tmp_sil)\n",
    "    sils.append(val)\n",
    "    sils_err.append(err)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
