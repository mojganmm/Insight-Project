{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries and dependencies\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from time import perf_counter \n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import osmnx as ox \n",
    "from collections import Counter\n",
    "import pygal\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, time\n",
    "import matplotlib\n",
    "\n",
    "#Interactive visualization\n",
    "import folium\n",
    "from folium import plugins\n",
    "# Adds tool to the top right\n",
    "from folium.plugins import MeasureControl\n",
    "import branca.colormap as cm\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import MultiPoint\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn import metrics\n",
    "\n",
    "from pygeocoder import Geocoder \n",
    "\n",
    "import reverse_geocoder as rg #info. on output:(name: cityname, admin1:state, admin2:county, cc:country code): https://github.com/thampiman/reverse-geocoder\n",
    "import pprint \n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from plotly import tools\n",
    "import matplotlib.cm as cm\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import random\n",
    "\n",
    "from kneed.data_generator import DataGenerator\n",
    "from kneed.knee_locator import KneeLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  # To Compute Haversine distance\n",
    "def sphere_dist(o_lat, o_lon, d_lat, d_lon):\n",
    "    \"\"\"\n",
    "    Return distance along great radius between origin and destination coordinates.\n",
    "    \"\"\"\n",
    "    #Define earth radius (km)\n",
    "    R_earth = 6371\n",
    "    #Convert degrees to radians\n",
    "    o_lat, o_lon, d_lat, d_lon = map(np.radians,\n",
    "                                                             [o_lat, o_lon, \n",
    "                                                              d_lat, d_lon])\n",
    "    #Compute distances along lat, lon dimensions\n",
    "    dlat = d_lat - o_lat\n",
    "    dlon = d_lon - o_lon\n",
    "    \n",
    "    #Compute haversine distance\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(o_lat) * np.cos(d_lat) * np.sin(dlon/2.0)**2\n",
    "    return 2 * R_earth * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def radian_conv(degree):\n",
    "    \"\"\"\n",
    "    Return radian.\n",
    "    \"\"\"\n",
    "    return  np.radians(degree) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Courtesy of https://stackoverflow.com/questions/26079881/kl-divergence-of-two-gmms. Here the difference is that we take the squared root, so it's a proper metric\n",
    "\n",
    "def gmm_js(gmm_p, gmm_q, n_samples=10**5):\n",
    "    X = gmm_p.sample(n_samples)[0]\n",
    "    log_p_X = gmm_p.score_samples(X)\n",
    "    log_q_X = gmm_q.score_samples(X)\n",
    "    log_mix_X = np.logaddexp(log_p_X, log_q_X)\n",
    "\n",
    "    Y = gmm_q.sample(n_samples)[0]\n",
    "    log_p_Y = gmm_p.score_samples(Y)\n",
    "    log_q_Y = gmm_q.score_samples(Y)\n",
    "    log_mix_Y = np.logaddexp(log_p_Y, log_q_Y)\n",
    "    \n",
    "    return np.sqrt((log_p_X.mean() - (log_mix_X.mean() - np.log(2))\n",
    "            + log_q_Y.mean() - (log_mix_Y.mean() - np.log(2))) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Training Dataset for the Model\n",
    "\n",
    "def ReadFromCSV(FilePath, sckipLines = 0, sep = ','):  \n",
    "\n",
    "    #Read and clean the csv file in FilePath ignoring the first sckipLines lines.\n",
    "\n",
    "    #(omits non-ascii characters from columns' name)\n",
    "\n",
    "    xa = pd.read_csv(FilePath, header=sckipLines, sep = sep, low_memory=False)\n",
    "\n",
    "    return xa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_manipulation(J):\n",
    "    \n",
    "    ## clean the data from non meaningful lat. and long.\n",
    "    # ##### Valid range of latitude and longitude for India: lat range( 8°4′ N to 37°6′ N) & long. range( 68°7′ E to 97°25′ E)\n",
    "    \n",
    "    J=J[(J['origin_lat'] >= 8) & (J['origin_lat'] <= 38)  & (J['destination_lat'] >= 8) & (J['destination_lat']<=38)]\n",
    "    J=J[(J['origin_lng'] >= 67) & (J['origin_lng'] <= 98)  & (J['destination_lng'] >= 67) & (J['destination_lng']<=98)]                            \n",
    "               \n",
    "    J_clean=J.loc[(J['origin_lng'] >= 67) & (J['destination_lng'] <= 98)]\n",
    "    \n",
    "    \n",
    "    # ### Jitter in lat/long of cols ==> The second decimal place is worth up to 1.1 km\n",
    "\n",
    "    J_clean['origin_lat']=J_clean['origin_lat'].map(lambda x: x +np.random.rand()/5.0)\n",
    "    J_clean['origin_lng']=J_clean['origin_lng'].map(lambda x: x +np.random.rand()/5.0)\n",
    "\n",
    "    J_clean['destination_lat']=J_clean['destination_lat'].map(lambda x: x +np.random.rand()/5.0)\n",
    "    J_clean['destination_lng']=J_clean['destination_lng'].map(lambda x: x +np.random.rand()/5.0)\n",
    "    \n",
    "    J_clean=J_clean.dropna()\n",
    "    J_clean['distance'] = sphere_dist(J_clean['origin_lat'], J_clean['origin_lng'], \n",
    "                                   J_clean['destination_lat'] , J_clean['destination_lng'])\n",
    "    J_clean[\"datetime\"] = pd.to_datetime(J_clean[\"datetime\"]) #for time series needed to be dtype: datetime64[ns, UTC]\n",
    "    \n",
    "    return J_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Long trip shipment or short trip shipment distances\n",
    "\n",
    "def vis_route_length_dist(d):\n",
    "\n",
    "    route_lengths=d['distance']\n",
    "\n",
    "    long_routes = len([k for k in route_lengths if k > 400]) / len(route_lengths)\n",
    "    medium_routes = len([k for k in route_lengths if k < 400 and k > 300]) / len(route_lengths)\n",
    "    short_routes = len([k for k in route_lengths if k < 300]) / len(route_lengths)\n",
    "\n",
    "\n",
    "    chart = pygal.HorizontalBar()\n",
    "    chart.title = 'Long, medium, and short routes'\n",
    "    chart.add('Long', long_routes * 100)\n",
    "    chart.add('Medium', medium_routes * 100)\n",
    "    chart.add('Short', short_routes * 100)\n",
    "    \n",
    "    return chart\n",
    "#vis_route_length_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #####. Function of find optimal number of clusters (GMM components)? Based on AIC\n",
    "\n",
    "\n",
    "def optimal_cluster_k (X):\n",
    "    \n",
    "    if X.shape[0]==1 or X.shape[0]==2 :\n",
    "        n_clusters=1\n",
    "    \n",
    "    elif 2< X.shape[0] <= 10:\n",
    "        n_clusters=2\n",
    "        \n",
    "        gmm = GaussianMixture(n_components=2, covariance_type='full',random_state=42).fit(X)\n",
    "        cluster_labels = gmm.predict(X)\n",
    "        \n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "\n",
    "        print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", silhouette_avg)    \n",
    "        \n",
    "    elif  11 <= X.shape[0] <= 100:\n",
    "        \n",
    "        k_clusters= np.arange(2, X.shape[0]-1,5)\n",
    "        models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(X) for n in k_clusters]\n",
    "        kn = KneeLocator(k_clusters, [m.aic(X) for m in models], curve='convex', direction='decreasing')\n",
    "        knee_k = kn.knee\n",
    "        \n",
    "        #print (knee_k)\n",
    "        \n",
    "        bic_min = min([m.bic(X) for m in models])\n",
    "        k_bic=k_clusters[[m.bic(X) for m in models].index(bic_min)]\n",
    "        \n",
    "        #print (k_bic)\n",
    "        \n",
    "#         plt.plot(k_clusters, [m.bic(X) for m in models], label='BIC') \n",
    "#         plt.plot(k_clusters, [m.aic(X) for m in models], label='AIC') \n",
    "#         plt.vlines(x=knee_k, ymin=min([m.bic(X)-500 for m in models]), ymax=max([m.bic(X) for m in models]), linestyle='--')\n",
    "#         plt.legend(loc='best')\n",
    "#         plt.xlabel('n_components', fontsize=15);\n",
    "\n",
    "        #start_r=np.min(k_bic,knee_k)\n",
    "        \n",
    "        range_n_clusters = np.arange(2, X.shape[0]-1,1)\n",
    "\n",
    "        for n_clusters in range_n_clusters:\n",
    "\n",
    "            # Initialize the clusterer with n_clusters value and a random generator\n",
    "            # seed of 10 for reproducibility.\n",
    "            #clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "            #cluster_labels = clusterer.fit_predict(X)\n",
    "            gmm = GaussianMixture(n_components=n_clusters, covariance_type='full',random_state=42).fit(X)\n",
    "            cluster_labels = gmm.predict(X)\n",
    "            print (cluster_labels)\n",
    "            # The silhouette_score gives the average value for all the samples.\n",
    "            # This gives a perspective into the density and separation of the formed\n",
    "            # clusters\n",
    "            \n",
    "            silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "     \n",
    "            if silhouette_avg > 0.5:\n",
    "                print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", silhouette_avg)    \n",
    "                break\n",
    "    elif X.shape[0] > 100:\n",
    "        \n",
    "        k_clusters= np.arange(2, 90,10)\n",
    "        models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(X) for n in k_clusters]\n",
    "        kn = KneeLocator(k_clusters, [m.aic(X) for m in models], curve='convex', direction='decreasing')\n",
    "        knee_k = kn.knee\n",
    "        print (knee_k)\n",
    "        \n",
    "        bic_min = min([m.bic(X) for m in models])\n",
    "        k_bic=k_clusters[[m.bic(X) for m in models].index(bic_min)]\n",
    "       \n",
    "        print (k_bic)\n",
    "        \n",
    "#         plt.plot(k_clusters, [m.bic(X) for m in models], label='BIC') \n",
    "#         plt.plot(k_clusters, [m.aic(X) for m in models], label='AIC') \n",
    "#         plt.vlines(x=knee_k, ymin=min([m.bic(X)-500 for m in models]), ymax=max([m.bic(X) for m in models]), linestyle='--')\n",
    "#         plt.legend(loc='best')\n",
    "#         plt.xlabel('n_components', fontsize=15);\n",
    "        \n",
    "        range_n_clusters = np.arange(knee_k, 90,2) #step was 2 before\n",
    "\n",
    "        for n_clusters in range_n_clusters:\n",
    "\n",
    "            # Initialize the clusterer with n_clusters value and a random generator\n",
    "            # seed of 10 for reproducibility.\n",
    "            #clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "            #cluster_labels = clusterer.fit_predict(X)\n",
    "            gmm = GaussianMixture(n_components=n_clusters, covariance_type='full',random_state=42).fit(X)\n",
    "            cluster_labels = gmm.predict(X)\n",
    "\n",
    "            # The silhouette_score gives the average value for all the samples.\n",
    "            # This gives a perspective into the density and separation of the formed\n",
    "            # clusters\n",
    "            silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "\n",
    "\n",
    "            if silhouette_avg > 0.5:\n",
    "                print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", silhouette_avg)    \n",
    "                break\n",
    "    return n_clusters #k_bic if None else knee_k #np.min(n_clusters, X.shape[0]) \n",
    "    \n",
    "#coords = df_train_dict['Telangana_Uttarakhand'].as_matrix(columns=['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng'])\n",
    "\n",
    "#optim_n= optimal_cluster_k(coords)\n",
    "#df_train_dict['Telangana_Uttarakhand'].shape[0] #25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_Model(df_train,outputpath)\n",
    "\n",
    "    #create new column to store origin state_destination state combination\n",
    "    df_train['shipments_states']=df_train.apply(lambda x:'%s_%s' % (x['origin_state'],x['destination_state']),axis=1) \n",
    "\n",
    "    #make a list from all the route in each state_state shipments\n",
    "    routes_states = df_train['shipments_states'].unique().tolist()\n",
    "\n",
    "    #create a dictionary to store all dataframes that each stores the single origin state_destination state\n",
    "    df_train_dict = {state_state: df_train.loc[df_train['shipments_states'] == state_state] for state_state in routes_states}\n",
    "\n",
    "\n",
    "    k_list=[]\n",
    "    gmm_tr={}\n",
    "\n",
    "    for k, v in df_train_dict.items():\n",
    "\n",
    "        if df_train_dict[k].shape[0]==1:\n",
    "            print (k)\n",
    "            print ('This route is single route and one cluster')\n",
    "        else:\n",
    "            print (k)\n",
    "            coords_tr = df_train_dict[k].as_matrix(columns=['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng'])\n",
    "\n",
    "            optim_tr= optimal_cluster_k(coords_tr)\n",
    "\n",
    "\n",
    "            gmm_tr[k]= GaussianMixture(n_components=optim_tr, covariance_type='full',random_state=42).fit(coords_tr)\n",
    "\n",
    "            cluster_labels_tr = gmm_tr[k].predict(coords_tr)      \n",
    "            probs_tr = gmm_tr[k].predict_proba(coords_tr)                  \n",
    "\n",
    "            df_train_dict[k]['route_zones']= pd.Series(cluster_labels_tr, index=df_train_dict[k].index)\n",
    "            df_train_dict[k]['probs_route_zone'] = pd.Series(probs_tr.max(1), index=df_train_dict[k].index)\n",
    "            df_train_dict[k].to_csv(outputpath + k + '.csv')\n",
    "\n",
    "\n",
    "    print ('k_list:',k_list)\n",
    "    \n",
    "    return \n",
    "\n",
    "    \n",
    "t1_start = perf_counter()\n",
    "t1_stop = perf_counter() \n",
    "  \n",
    "print(\"Elapsed time:\", t1_stop, t1_start)  \n",
    "print(\"Elapsed time during the whole program in minutes:\",(t1_stop-t1_start)/60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all=ReadFromCSV('/Users/mojganmazouchi/Desktop/golorry_route/all_data.csv') \n",
    "df=data_manipulation(df_all) #(107498, 11)\n",
    "#df.shape\n",
    "\n",
    "GMM_Model(df,/Results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For visualization \n",
    "\n",
    "### get subset of state to state route which you want to visualize after building the model\n",
    "\n",
    "def state_state_clusters(ss, data_dict):\n",
    "\n",
    "    #ss='West Bengal_Telangana' ## input the origin state to destination state with this example format 'Telangana_West Bengal'\n",
    "                                                              \n",
    "    # # mark each origin as a point\n",
    "    df_plot=df_dict[ssr] ## 22 zones detected\n",
    "\n",
    "    all_routes = df_plot['route_zones'].unique().tolist()\n",
    "\n",
    "    df_dict_zones = {route_z: df_plot.loc[df_plot['route_zones'] == route_z] for route_z in all_routes}\n",
    "\n",
    "\n",
    "    m = folium.Map([16.7, 81.095], zoom_start=11,control_scale = True)\n",
    "\n",
    "\n",
    "    #create a color map\n",
    "    #color_var = 'route_zones' #what variable will determine the color\n",
    "    cmap = cm.LinearColormap(['blue', 'red','green', 'orange','pink','black', 'beige', 'white','lightblue','purple','gray','cadetblue'],\n",
    "                         vmin=df_plot['route_zones'].quantile(0.05), vmax=df_plot['route_zones'].quantile(0.95),\n",
    "                         caption = 'route_zones')\n",
    "\n",
    "    #Add the color map legend to your map\n",
    "    m.add_child(cmap)\n",
    "    #Add the measurement control bar\n",
    "    m.add_child(MeasureControl())\n",
    "\n",
    "  \n",
    "        ####### Source info. on Folium https://www.kaggle.com/rachan/how-to-folium-for-maps-heatmaps-time-analysis\n",
    "    for k, v in df_dict_zones.items():\n",
    "\n",
    "\n",
    "        for index, row in df_dict_zones[k].iterrows():#folium.Marker([lat, lon], popup=str(name)+': '+color+'-'+str(clname), icon=folium.Icon(color=color)).add_to(feature_group)\n",
    "\n",
    "            folium.Marker([row['origin_lat'], row['origin_lng']],\n",
    "                                radius=4,fill=True, # Set fill to True\n",
    "                                fill_color=\"green\",fill_opacity=0.7).add_to(m)\n",
    "\n",
    "        for index, row in df_dict_zones[k].iterrows():\n",
    "\n",
    "            folium.Marker([row['destination_lat'], row['destination_lng']],\n",
    "                                radius=7,\n",
    "                                fill=True, # Set fill to True\n",
    "                                color = 'red',\n",
    "                                fill_opacity=0.7).add_to(m)\n",
    "\n",
    "            folium.PolyLine([[row['origin_lat'], row['origin_lng']], [row['destination_lat'], row['destination_lng']]],color=cmap(row['route_zones']),line_weight=5).add_to(m)\n",
    "    \n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Check the robustness of a model with shuffle data for specific state_state route\n",
    "\n",
    "def Robustness_check(ss, df_p,df_q):\n",
    "\n",
    "df_tr=df\n",
    "\n",
    "df_check_p=df_tr.copy()\n",
    "df_check_q=df_tr.sample(n=55000)\n",
    "\n",
    "shipment_route='West Bengal_Telangana' ## input the origin state to destination state with this example format 'Telangana_West Bengal'\n",
    "  \n",
    "df_check_p['shipments_states']=df_check_p.apply(lambda x:'%s_%s' % (x['origin_state'],x['destination_state']),axis=1)\n",
    "#routes_states = df_check_p['shipments_states'].unique().tolist()\n",
    "routes_states = [shipment_route]#'Andhra Pradesh_Andhra Pradesh'\n",
    "\n",
    "df_dict_p = {state_state: df_check_p.loc[df_check_p['shipments_states'] == state_state] for state_state in routes_states}\n",
    "\n",
    "\n",
    "df_check_q['shipments_states']=df_check_q.apply(lambda x:'%s_%s' % (x['origin_state'],x['destination_state']),axis=1)\n",
    "df_dict_q = {state_state: df_check_q.loc[df_check_q['shipments_states'] == state_state] for state_state in routes_states}\n",
    "\n",
    "\n",
    "p_list=[]\n",
    "q_list=[]\n",
    "k_list=[]\n",
    "JSD=[]\n",
    "gmm_p={}\n",
    "gmm_q={}\n",
    "js_dist={}\n",
    "#for k, v in df_dict.items():\n",
    "for (k,v), (k2,v2) in zip(df_dict_p.items(), df_dict_q.items()):\n",
    "    \n",
    "    if df_dict_p[k].shape[0]==1 & df_dict_q[k2].shape[0]==1:\n",
    "        \n",
    "        print ('This route is single route and one cluster')\n",
    "    else:\n",
    "        \n",
    "        coords_p = df_dict_p[k].as_matrix(columns=['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng'])\n",
    "        coords_q = df_dict_q[k2].as_matrix(columns=['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng'])\n",
    "        \n",
    "        optim_p= optimal_cluster_k(coords_p)\n",
    "        optim_q= optimal_cluster_k(coords_q)\n",
    "        \n",
    "     \n",
    "        gmm_p[k]= GaussianMixture(n_components=optim_p, covariance_type='full',random_state=42).fit(coords_p)\n",
    "        gmm_q[k2]= GaussianMixture(n_components=optim_q, covariance_type='full',random_state=42).fit(coords_q)\n",
    "        \n",
    "        cluster_labels_p = gmm_p[k].predict(coords_p)\n",
    "        cluster_labels_q = gmm_p[k2].predict(coords_q)\n",
    "        \n",
    "        probs_p = gmm_p[k].predict_proba(coords_p)\n",
    "        probs_q = gmm_q[k2].predict_proba(coords_q) \n",
    "        \n",
    "        js_dist[k]=gmm_js(gmm_p[k], gmm_q[k2], n_samples=100)\n",
    "        \n",
    "        if optim_p!= optim_q:\n",
    "\n",
    "            p_list.append(optim_p)\n",
    "            q_list.append(optim_q)\n",
    "            k_list.append(k)\n",
    "            JSD.append(js_dist[k])            \n",
    "\n",
    "        #print ('js',js_dist)\n",
    "        df_dict_p[k]['route_zones']= pd.Series(cluster_labels_p, index=df_dict_p[k].index)\n",
    "        df_dict_p[k]['probs_route_zone'] = pd.Series(probs_p.max(1), index=df_dict_p[k].index)\n",
    "        #df_dict[k].to_csv(k + '.csv')\n",
    "         \n",
    "print ('p_list:', p_list)\n",
    "print ('q_list:', q_list)\n",
    "print ('k_list:',k_list)\n",
    "print ('JS distance:', js_dist[k])\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
